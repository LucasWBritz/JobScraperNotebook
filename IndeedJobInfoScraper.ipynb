{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import necessary modules\n",
    "import time # to be able to run the sleep function\n",
    "import csv\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import this to access shared functions allowing us to share code across notebooks\n",
    "import sharedFunctions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function receives a page an extracts the total jobs count.\n",
    "def getIndeedSearchCountPages(parsed_html):\n",
    "    section = parsed_html.body.find('div', attrs={'id':'searchCountPages'}).text\n",
    "    text = [t for t in section.split(' ') if t != '' and t != '\\n'] # removes empty spaces. \n",
    "    return int(re.sub(\"[^0-9]\", \"\", text[3])) #remove all non numeric chars from this string "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function will receive a single page then extract all job items. \n",
    "def parseIndeedPage(parsed_html):\n",
    "    return parsed_html.body.find_all('div', attrs={'class':'title'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to receive a listing page, then download all job links.\n",
    "def scrapeSearchPage(queryLink, baseUrl):\n",
    "    print(\"Starting with \" + queryLink)\n",
    "    mainPage = sharedFunctions.downloadAndParseLink(queryLink)\n",
    "    totalItemsCount = getIndeedSearchCountPages(mainPage)\n",
    "    \n",
    "    print(\"Processing for {0} items\".format(totalItemsCount))\n",
    "    if totalItemsCount > 0:\n",
    "        totalProcessed = 0\n",
    "        while totalProcessed < totalItemsCount:\n",
    "            try:\n",
    "                link = queryLink + \"&start=\" + str(totalProcessed)            \n",
    "                sections = None\n",
    "                if totalProcessed == 0:\n",
    "                    print(\"Processing main page\")\n",
    "                    sections = parseIndeedPage(mainPage)\n",
    "                else:\n",
    "                    print(\"Downloading {0}\".format(link))\n",
    "                    sections = parseIndeedPage(sharedFunctions.downloadAndParseLink(link))                \n",
    "                \n",
    "                for section in sections:\n",
    "                    lnk = baseUrl + sharedFunctions.getAttrFromTag(section.a, 'href')\n",
    "                    if lnk not in allJobLinks:\n",
    "                        allJobLinks.append(lnk)\n",
    "            \n",
    "                totalProcessed += 10 #each page has 10 links.            \n",
    "                #wait for 2 seconds\n",
    "                time.sleep(2)\n",
    "            except: \n",
    "                print(\"Exception happened\")\n",
    "                #sometimes I noticed some internal errors like error 500, so lets wait a little bit more\n",
    "                print(\"Waiting 10 seconds\")\n",
    "                time.sleep(10)\n",
    "                pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This method receives a link and extract the needed info, returning a tuple. \n",
    "def downloadAndCreateObject(link):\n",
    "    try:\n",
    "        print(\"Downloading {0}\".format(link))\n",
    "        soup = sharedFunctions.downloadAndParseLink(link)\n",
    "    \n",
    "        companyName = soup.find('div', attrs={'class':'icl-u-lg-mr--sm'}).text.encode(\"utf-8\")\n",
    "        title = soup.find('h3', attrs={'class':'jobsearch-JobInfoHeader-title'}).text.encode(\"utf-8\")\n",
    "        location = soup.find('span', attrs={'class':'jobsearch-JobMetadataHeader-iconLabel'}).text.encode(\"utf-8\")\n",
    "        \n",
    "        return (title, companyName, location, link)\n",
    "    except:\n",
    "        print(\"Waiting 10 seconds cause of an exception\")\n",
    "        time.sleep(10)        \n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this list will store all the job links scraped\n",
    "allJobLinks = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scrapeSearchPage(\"https://ca.indeed.com/jobs?q=remote+developer\", \"https://ca.indeed.com\")\n",
    "scrapeSearchPage(\"https://ca.indeed.com/jobs?q=java+developer\", \"https://ca.indeed.com\")\n",
    "scrapeSearchPage(\"https://ca.indeed.com/jobs?q=.net+developer\", \"https://ca.indeed.com\")\n",
    "scrapeSearchPage(\"https://ca.indeed.com/jobs?q=ruby+on+rails\", \"https://ca.indeed.com\")\n",
    "scrapeSearchPage(\"https://ca.indeed.com/jobs?q=machine+learning\", \"https://ca.indeed.com\")\n",
    "scrapeSearchPage(\"https://ca.indeed.com/jobs?q=react+developer\", \"https://ca.indeed.com\")\n",
    "scrapeSearchPage(\"https://ca.indeed.com/jobs?q=mobile+developer\", \"https://ca.indeed.com\")\n",
    "scrapeSearchPage(\"https://ca.indeed.com/jobs?q=salesforce\", \"https://ca.indeed.com\")\n",
    "scrapeSearchPage(\"https://ca.indeed.com/jobs?q=database+administrator\", \"https://ca.indeed.com\")\n",
    "scrapeSearchPage(\"https://ca.indeed.com/jobs?q=game+developer\", \"https://ca.indeed.com\")\n",
    "scrapeSearchPage(\"https://ca.indeed.com/jobs?q=python+developer\", \"https://ca.indeed.com\")\n",
    "scrapeSearchPage(\"https://ca.indeed.com/jobs?q=data+scientist\", \"https://ca.indeed.com\")\n",
    "scrapeSearchPage(\"https://ca.indeed.com/jobs?q=devops\", \"https://ca.indeed.com\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"{0} jobs found!\".format(len(allJobLinks)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#will store all the job info retrieved until we save it to a csv file\n",
    "parsedInfo = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "downloaded = 0\n",
    "for job in allJobLinks:\n",
    "    #scrape this link info\n",
    "    item = downloadAndCreateObject(job)\n",
    "    if item != None:\n",
    "        parsedInfo.append(item)\n",
    "        downloaded += 1\n",
    "        print(\"{0} downloaded of {1}\".format(downloaded, len(allJobLinks)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('JobInfo.csv','w') as out:\n",
    "    csv_out=csv.writer(out)\n",
    "    csv_out.writerow(['PositionTitle','CompanyName', 'Location', 'JobLink'])\n",
    "    for row in parsedInfo:\n",
    "        csv_out.writerow(row)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
